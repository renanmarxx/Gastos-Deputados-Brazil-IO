name: Main workflow for CI/CD validations

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: "us-east-1"

permissions:
  id-token: write
  contents: read

jobs:
  AssumeRoleAndCallIdentity:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Branch
        uses: actions/checkout@v3

      - name: Configure AWS Credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: arn:aws:iam::535942231592:role/GitHub_Actions_Personal
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Use credentials directly
        run: |
          export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
          export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN

          echo "Temporary valid credentials sent to Databricks environment"

      - name: STS GetCallerIdentity
        run: aws sts get-caller-identity

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.6'
          architecture: 'x64'

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          pip install poetry

      - name: Cache Poetry and pip
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            ~/.cache/pip
          key: ${{ runner.os }}-poetry-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-poetry-

      - name: Install dependencies (Poetry)
        run: |
          poetry install --no-interaction --no-ansi --no-root
      
      - name: Activating Poetry Shell (virtual environment)
        run: |
          poetry env activate

      - name: Run black on python files
        run: |
          poetry run black scripts/brasil_io.py

      - name: Generate CSV
        env:
          BRASIL_IO_TOKEN: ${{ secrets.BRASIL_IO_TOKEN }}
        run: |
          poetry run python scripts/brasil_io.py

      - name: Upload to S3 
        run: |
          aws s3 cp data/gastos-deputados_cota_parlamentar.csv.gz ${{ secrets.LANDING_BUCKET_INGESTION }}
           
      - name: Trigger Databricks Ingestion Job with STS credentials
        run: |
          curl -X POST "${{ secrets.DATABRICKS_WORKSPACE_URL }}/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer ${{ secrets.DATABRICKS_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d "{
              \"job_id\": \"${{ secrets.DATABRICKS_JOB_ID }}\",
              \"task_key\": \"test\",
              \"notebook_params\": {
                \"aws_access_key\": \"${AWS_ACCESS_KEY_ID}\",
                \"aws_secret_key\": \"${AWS_SECRET_ACCESS_KEY}\",
                \"aws_session_token\": \"${AWS_SESSION_TOKEN}\"
              }
            }"
